{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Recall that $ n! $ is read as “$ n $ factorial” and defined as\n",
    "$ n! = n \\times (n - 1) \\times \\cdots \\times 2 \\times 1 $\n",
    "\n",
    "In Julia you can compute this value with `factorial(n)`\n",
    "\n",
    "Write your own version of this function, called `factorial2`, using a `for` loop\n",
    "\n",
    "\n",
    "<a id='jbe-ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "The [binomial random variable](https://en.wikipedia.org/wiki/Binomial_distribution) $ Y \\sim Bin(n, p) $ represents\n",
    "\n",
    "- number of successes in $ n $ binary trials  \n",
    "- each trial succeeds with probability $ p $  \n",
    "\n",
    "\n",
    "Using only `rand()` from the set of Julia’s built-in random number\n",
    "generators (not the `Distributions` package), write a function `binomial_rv` such that `binomial_rv(n, p)` generates one draw of $ Y $\n",
    "\n",
    "Hint: If $ U $ is uniform on $ (0, 1) $ and $ p \\in (0,1) $, then the expression `U < p` evaluates to `true` with probability $ p $\n",
    "\n",
    "\n",
    "<a id='jbe-ex3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Compute an approximation to $ \\pi $ using Monte Carlo\n",
    "\n",
    "For random number generation use only `rand()`\n",
    "\n",
    "Your hints are as follows:\n",
    "\n",
    "- If $ U $ is a bivariate uniform random variable on the unit square $ (0, 1)^2 $, then the probability that $ U $ lies in a subset $ B $ of $ (0,1)^2 $ is equal to the area of $ B $  \n",
    "- If $ U_1,\\ldots,U_n $ are iid copies of $ U $, then, as $ n $ gets larger, the fraction that falls in $ B $ converges to the probability of landing in $ B $  \n",
    "- For a circle, area = π * $ radius^2 $  \n",
    "\n",
    "\n",
    "\n",
    "<a id='jbe-ex4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Write a program that prints one realization of the following random device:\n",
    "\n",
    "- Flip an unbiased coin 10 times  \n",
    "- If 3 consecutive heads occur one or more times within this sequence, pay one dollar  \n",
    "- If not, pay nothing  \n",
    "\n",
    "\n",
    "Once again use only `rand()` as your random number generator\n",
    "\n",
    "\n",
    "<a id='jbe-ex5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "Simulate and plot the correlated time series\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\alpha \\, x_t + \\epsilon_{t+1}\n",
    "\\quad \\text{where} \\quad\n",
    "x_0 = 0\n",
    "\\quad \\text{and} \\quad t = 0,\\ldots,n\n",
    "$$\n",
    "\n",
    "The sequence of shocks $ \\{\\epsilon_t\\} $ is assumed to be iid and standard normal\n",
    "\n",
    "Set $ n = 200 $ and $ \\alpha = 0.9 $\n",
    "\n",
    "\n",
    "<a id='jbe-ex6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 6\n",
    "\n",
    "Plot three simulated time series, one for each of the cases $ \\alpha = 0 $, $ \\alpha = 0.8 $ and $ \\alpha = 0.98 $\n",
    "\n",
    "(The figure will illustrate how time series with the same one-step-ahead conditional volatilities, as these three processes have, can have very different unconditional volatilities)\n",
    "\n",
    "\n",
    "<a id='jbe-ex7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 7\n",
    "\n",
    "This exercise is more challenging\n",
    "\n",
    "Take a random walk, starting from $ x_0 = 1 $\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\, \\alpha \\, x_t + \\sigma\\, \\epsilon_{t+1}\n",
    "\\quad \\text{where} \\quad\n",
    "x_0 = 1\n",
    "\\quad \\text{and} \\quad t = 0,\\ldots,t_{\\max}\n",
    "$$\n",
    "\n",
    "- Furthermore, assume that the $ x_{t_{\\max}} = 0 $  (i.e. at $ t_{\\max} $, the value drops to zero, regardless of its current state)  \n",
    "- The sequence of shocks $ \\{\\epsilon_t\\} $ is assumed to be iid and standard normal  \n",
    "- For a given path $ \\{x_t\\} $ define a **first-passage time** as $ T_a = \\min\\{t\\, |\\, x_t \\leq a\\} $, where by the assumption of the process $ T_a \\leq t_{\\max} $  \n",
    "\n",
    "\n",
    "Start with $ \\sigma = 0.2, \\alpha = 1.0 $\n",
    "\n",
    "1. calculate the first-passage time, $ T_0 $, for 100 simulated random walks – to a $ t_{\\max} = 200 $ and plot a histogram  \n",
    "1. plot the sample mean of $ T_0 $ from the simulation for $ \\alpha \\in \\{0.8, 1.0, 1.2\\} $  \n",
    "\n",
    "\n",
    "\n",
    "<a id='jbe-ex8a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 8(a)\n",
    "\n",
    "This exercise is more challenging\n",
    "\n",
    "The root of a univariate function $ f(\\cdot) $ is an $ x $ such that $ f(x) = 0 $\n",
    "\n",
    "One solution method to find local roots of smooth functions is called Newton’s method\n",
    "\n",
    "Starting with an $ x_0 $ guess, a function $ f(\\cdot) $ and the first-derivative $ f'(\\cdot) $, the algorithm is to repeat\n",
    "\n",
    "$$\n",
    "x^{n+1} = x^n - \\frac{f(x^n)}{f'(x^n)}\n",
    "$$\n",
    "\n",
    "until $ | x^{n+1} - x^n| $ is below a tolerance\n",
    "\n",
    "1. Use a variation of the `fixedpointmap` code to implement Newton’s method, where the function would accept arguments `f, f_prime, x_0, tolerance, maxiter`  \n",
    "1. Test it with $ f(x) = (x-1)^3 $ and another function of your choice where you can analytically find the derivative  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 8(b)\n",
    "\n",
    "For those impatient to use more advanced features of Julia, implement a version of Exercise 8(a) where `f_prime` is calculated with auto-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010000000000000002, 0.2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "# operator to get the derivative of this function using AD\n",
    "D(f) = x -> ForwardDiff.derivative(f, x)\n",
    "\n",
    "# example usage: create a function and get the derivative\n",
    "f(x) = x^2\n",
    "f_prime = D(f)\n",
    "\n",
    "f(0.1), f_prime(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Using the `D(f)` operator definition above, implement a version of Newton’s method that does not require the user to provide an analytical derivative  \n",
    "1. Test the sorts of `f` functions which can be automatically integrated by `ForwardDff.jl`  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "filename": "julia_by_example.rst",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  },
  "title": "Introductory Examples"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
